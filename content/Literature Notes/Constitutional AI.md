* [[Helpfulness]] and [[Harmlessness]] can often be in conflict
	* ie. chat GPT won't tell you how to take over the world (harmless) but this system often misfires and gives you a generic "I can't help you" message (not helpful)
* AI supervision of models can allow you to define a "constitution" of principles for an AI to follow 
* The main gist of the paper is removing AI supervision for harmlessness

Reading path:
* [ ] Chain of thought reasoning - https://arxiv.org/abs/2112.00114
* [ ] Red teaming language models with language models - https://arxiv.org/abs/2202.03286
* [ ] General RLHF papers - 
	* [ ] Learning to summarize from human feedback - https://arxiv.org/abs/2009.01325
	* [ ] Training a helpful and harmless assistant with reinforcement learning from human feedback. - https://arxiv.org/abs/2204.05862
	* [ ] Training language models to follow instructions with human feedback - https://arxiv.org/abs/2203.02155
*source: 
* https://www.anthropic.com/constitutional.pdf
* https://github.com/anthropics/ConstitutionalHarmlessnessPaper
